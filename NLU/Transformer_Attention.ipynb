{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O7M6T_HurHnq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # ignore warnings\n",
        "np.set_printoptions(precision=4, suppress=True) # decimals to 4 places"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embedding creation"
      ],
      "metadata": {
        "id": "3KoKYjBwrgkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define embedding dimension\n",
        "embedding_dim = 8 # constant/scalar used to describe each word, they use 512 dimensions in the paper\n",
        "\n",
        "# Randomly initialize word embeddings of size [embedding_dim]\n",
        "np.random.seed(42)  # For reproducibility\n",
        "word_embeddings = {\n",
        "    'you': np.random.rand(embedding_dim), # vector with random numbers\n",
        "    'are': np.random.rand(embedding_dim),\n",
        "    'amazing': np.random.rand(embedding_dim)\n",
        "}\n",
        "\n",
        "print(f\"Word Embedding shape: {word_embeddings['amazing'].shape}\")\n",
        "print(f\"Word Embedding for 'amazing': {word_embeddings['amazing']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUumGXworii5",
        "outputId": "cc5061f6-6ee9-42e6-881e-c5c0b340c32f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Embedding shape: (8,)\n",
            "Word Embedding for 'amazing': [0.3042 0.5248 0.4319 0.2912 0.6119 0.1395 0.2921 0.3664]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the next layer, positional encondings"
      ],
      "metadata": {
        "id": "t_nfOuHStxJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to generate positional encoding for each index and dim\n",
        "def positional_encoding(position, dim):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (np.arange(dim) // 2)) / np.float32(dim))\n",
        "    encoding = np.zeros(dim)\n",
        "    encoding[0::2] = np.sin(position * angle_rates[0::2])  # Even\n",
        "    encoding[1::2] = np.cos(position * angle_rates[1::2])  # Odd\n",
        "    return encoding\n",
        "\n",
        "# Add positional encodings to each word embedding\n",
        "positions = {'you': 0, 'are': 1, 'amazing': 2}\n",
        "for word, position in positions.items():\n",
        "    pos_encoding = positional_encoding(position, embedding_dim)\n",
        "    word_embeddings[word] = word_embeddings[word] + pos_encoding\n",
        "\n",
        "# Print results\n",
        "print(f\"Word Embedding shape: {word_embeddings['amazing'].shape}\")\n",
        "print(f\"Word Embedding for 'amazing' with positional encoding: {word_embeddings['amazing']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArEVOvmst2Fz",
        "outputId": "f1312970-fd10-451e-f395-1d7e55a3d61e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Embedding shape: (8,)\n",
            "Word Embedding for 'amazing' with positional encoding: [1.2135 0.1086 0.6306 1.2713 0.6319 1.1393 0.2941 1.3664]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuJUrrLEvORn",
        "outputId": "f3cf4554-6d3d-4445-fcb1-e2df4ebb42aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'you': array([0.3745, 1.9507, 0.732 , 1.5987, 0.156 , 1.156 , 0.0581, 1.8662]),\n",
              " 'are': array([1.4426, 1.2484, 0.1204, 1.9649, 0.8424, 1.2123, 0.1828, 1.1834]),\n",
              " 'amazing': array([1.2135, 0.1086, 0.6306, 1.2713, 0.6319, 1.1393, 0.2941, 1.3664])}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stack word embeddings into matrix"
      ],
      "metadata": {
        "id": "eG6fyS4cvPJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input matrix (stacked embeddings)\n",
        "X = np.vstack(list(word_embeddings.values()))\n",
        "\n",
        "seq_len = X.shape[0]\n",
        "\n",
        "print(f\"\\nSequence length: {seq_len}\")\n",
        "print(f\"\\nInput Matrix (X) shape:{X.shape}\") #Size: seq_len x embedding_dim\n",
        "print(f\"\\nInput Matrix (X):\\n{X}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5LhkyWtvYpH",
        "outputId": "51b40623-c967-4d5f-adf6-ff5c77348c57"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sequence length: 3\n",
            "\n",
            "Input Matrix (X) shape:(3, 8)\n",
            "\n",
            "Input Matrix (X):\n",
            "[[0.3745 1.9507 0.732  1.5987 0.156  1.156  0.0581 1.8662]\n",
            " [1.4426 1.2484 0.1204 1.9649 0.8424 1.2123 0.1828 1.1834]\n",
            " [1.2135 0.1086 0.6306 1.2713 0.6319 1.1393 0.2941 1.3664]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize random values for Q, K and V\n",
        "head_dim = embedding_dim # single head\n",
        "\n",
        "# Randomly initialize weight matrices for Q, K, V\n",
        "W_q = np.random.rand(embedding_dim, head_dim) # Shape (embedding_dim, head_dim)\n",
        "W_k = np.random.rand(embedding_dim, head_dim)\n",
        "W_v = np.random.rand(embedding_dim, head_dim)\n",
        "\n",
        "print(\"\\nRandomly Initialized Weight Matrices:\")\n",
        "print(f\"W_q:\\n{W_q}\")\n",
        "print(f\"W_k:\\n{W_k}\")\n",
        "print(f\"W_v:\\n{W_v}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Shape of W_q, W_k, W_v:\\n{W_q.shape},\\n{W_k.shape},\\n{W_v.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypzbcnK5vosZ",
        "outputId": "7ec6af91-d7fd-46d4-f758-392a677c19df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Randomly Initialized Weight Matrices:\n",
            "W_q:\n",
            "[[0.4561 0.7852 0.1997 0.5142 0.5924 0.0465 0.6075 0.1705]\n",
            " [0.0651 0.9489 0.9656 0.8084 0.3046 0.0977 0.6842 0.4402]\n",
            " [0.122  0.4952 0.0344 0.9093 0.2588 0.6625 0.3117 0.5201]\n",
            " [0.5467 0.1849 0.9696 0.7751 0.9395 0.8948 0.5979 0.9219]\n",
            " [0.0885 0.196  0.0452 0.3253 0.3887 0.2713 0.8287 0.3568]\n",
            " [0.2809 0.5427 0.1409 0.8022 0.0746 0.9869 0.7722 0.1987]\n",
            " [0.0055 0.8155 0.7069 0.729  0.7713 0.074  0.3585 0.1159]\n",
            " [0.8631 0.6233 0.3309 0.0636 0.311  0.3252 0.7296 0.6376]]\n",
            "W_k:\n",
            "[[0.8872 0.4722 0.1196 0.7132 0.7608 0.5613 0.771  0.4938]\n",
            " [0.5227 0.4275 0.0254 0.1079 0.0314 0.6364 0.3144 0.5086]\n",
            " [0.9076 0.2493 0.4104 0.7556 0.2288 0.077  0.2898 0.1612]\n",
            " [0.9297 0.8081 0.6334 0.8715 0.8037 0.1866 0.8926 0.5393]\n",
            " [0.8074 0.8961 0.318  0.1101 0.2279 0.4271 0.818  0.8607]\n",
            " [0.007  0.5107 0.4174 0.2221 0.1199 0.3376 0.9429 0.3232]\n",
            " [0.5188 0.703  0.3636 0.9718 0.9624 0.2518 0.4972 0.3009]\n",
            " [0.2848 0.0369 0.6096 0.5027 0.0515 0.2786 0.9083 0.2396]]\n",
            "W_v:\n",
            "[[0.1449 0.4895 0.9857 0.2421 0.6721 0.7616 0.2376 0.7282]\n",
            " [0.3678 0.6323 0.6335 0.5358 0.0903 0.8353 0.3208 0.1865]\n",
            " [0.0408 0.5909 0.6776 0.0166 0.5121 0.2265 0.6452 0.1744]\n",
            " [0.6909 0.3867 0.9367 0.1375 0.3411 0.1135 0.9247 0.8773]\n",
            " [0.2579 0.66   0.8172 0.5552 0.5297 0.2419 0.0931 0.8972]\n",
            " [0.9004 0.6331 0.339  0.3492 0.726  0.8971 0.8871 0.7799]\n",
            " [0.642  0.0841 0.1616 0.8986 0.6064 0.0092 0.1015 0.6635]\n",
            " [0.0051 0.1608 0.5487 0.6919 0.652  0.2243 0.7122 0.2372]]\n",
            "\n",
            "\n",
            "Shape of W_q, W_k, W_v:\n",
            "(8, 8),\n",
            "(8, 8),\n",
            "(8, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Q, K, V  dot prodct of our matrix X with our weights\n",
        "Q = np.dot(X, W_q)\n",
        "print(f\"\\nQuery Matrix (Q):\\n{Q}\")\n",
        "\n",
        "K = np.dot(X, W_k)\n",
        "print(f\"\\nKey Matrix (K):\\n{K}\")\n",
        "\n",
        "V = np.dot(X, W_v)\n",
        "print(f\"\\nValue Matrix (V):\\n{V}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jC0mJXyiyvAU",
        "outputId": "1b0942ac-674b-40d4-d41e-cdb6c14ced88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query Matrix (Q):\n",
            "[[3.2106 4.6715 4.3622 4.8134 3.2794 3.9177 5.1507 4.2588]\n",
            " [3.2656 4.4498 4.1326 4.8386 4.0389 3.8503 5.5063 3.9866]\n",
            " [2.8894 3.4368 2.4507 3.6915 3.0919 3.3843 4.2741 3.1117]]\n",
            "\n",
            "Key Matrix (K):\n",
            "[[4.1983 3.3252 3.0982 3.6923 2.1246 2.7978 5.4825 3.1297]\n",
            " [4.9891 4.3791 3.06   4.1014 3.3176 3.1249 6.2913 3.8818]\n",
            " [3.9476 3.2093 2.8283 3.7569 2.7267 2.1453 5.2656 2.7697]]\n",
            "\n",
            "Value Matrix (V):\n",
            "[[3.034  3.6074 5.1513 3.2015 3.5217 3.7557 5.0402 3.6896]\n",
            " [3.4629 3.8557 5.9133 3.1644 4.0228 3.9501 4.6531 5.1316]\n",
            " [2.5045 2.9097 4.583  2.4956 3.8129 2.7861 3.9781 4.104 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Claculate the attention scores"
      ],
      "metadata": {
        "id": "WMuV1my2zWl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = np.dot(Q, K.T)\n",
        "\n",
        "print(f\"\\nAttention Scores (QK^T):\\n{scores}\")\n",
        "print(f\"\\nShape of Attention Scores matrix:{scores.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSWHLt5GzPy-",
        "outputId": "429fa247-312c-4b1c-db90-704445a4c665"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attention Scores (QK^T):\n",
            "[[119.7964 141.6248 114.3513]\n",
            " [121.1941 143.8183 116.3466]\n",
            " [ 93.9901 111.9071  90.0504]]\n",
            "\n",
            "Shape of Attention Scores matrix:(3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale scores by sqrt(d_k)\n",
        "d_k = K.shape[1]\n",
        "\n",
        "scaled_scores = scores / np.sqrt(d_k)\n",
        "\n",
        "print(f\"\\nd_k (dimension of the K matrix):{d_k}\")\n",
        "print(f\"\\nScaled Attention Scores:\\n{scaled_scores}\")\n",
        "print(f\"\\nShape of Scaled Attention Scores:{scaled_scores.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FthtAUg50S7z",
        "outputId": "58662f49-e1e9-4672-93b0-d38b798fce71"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "d_k (dimension of the K matrix):8\n",
            "\n",
            "Scaled Attention Scores:\n",
            "[[42.3544 50.0719 40.4293]\n",
            " [42.8486 50.8474 41.1347]\n",
            " [33.2305 39.5651 31.8376]]\n",
            "\n",
            "Shape of Scaled Attention Scores:(3, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add attention Mask"
      ],
      "metadata": {
        "id": "1trEyjgx0Y89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.tri(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQvyvxzY0bLQ",
        "outputId": "7b25e13f-7878-4c77-95a6-78e52333bc32"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [1., 1., 0.],\n",
              "       [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mask, upper right triangle set to -inf\n",
        "mask = np.log(np.tri(3))\n",
        "\n",
        "# Apply mask to attention scores\n",
        "masked_scores = scaled_scores + mask  # adding -inf makes them 0 after softmax\n",
        "\n",
        "# For reference, mask looks like:\n",
        "print(\"Mask:\\n\", mask)\n",
        "print(\"\\n\")\n",
        "print(\"Masked Attention Scores\\n\", masked_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBfcmRVr0iR2",
        "outputId": "53c99a78-03fc-41e0-da37-d52b6ed290b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask:\n",
            " [[  0. -inf -inf]\n",
            " [  0.   0. -inf]\n",
            " [  0.   0.   0.]]\n",
            "\n",
            "\n",
            "Masked Attention Scores\n",
            " [[42.3544    -inf    -inf]\n",
            " [42.8486 50.8474    -inf]\n",
            " [33.2305 39.5651 31.8376]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying softmax"
      ],
      "metadata": {
        "id": "WIFFqJre0pCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1crM5Sx0rH_",
        "outputId": "8dce480a-93a5-4756-c78d-ee82c90cb9b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[42.3544,    -inf,    -inf],\n",
              "       [42.8486, 50.8474,    -inf],\n",
              "       [33.2305, 39.5651, 31.8376]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply softmax to get attention weights\n",
        "exp_scores = np.exp(masked_scores) # np.exp\n",
        "\n",
        "attention_weights = exp_scores / exp_scores.sum(axis=1, keepdims=True)\n",
        "print(f\"\\nAttention Weights (softmax):\\n{attention_weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZxtldTS0u4i",
        "outputId": "bb92c7ce-95ef-46db-bc64-02112737d853"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Attention Weights (softmax):\n",
            "[[1.     0.     0.    ]\n",
            " [0.0003 0.9997 0.    ]\n",
            " [0.0018 0.9978 0.0004]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute attention output"
      ],
      "metadata": {
        "id": "fbU8ku4403T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute attention output\n",
        "attention_output = np.dot(attention_weights, V) # (3x3).(3x8) np.dot\n",
        "\n",
        "print(f\"\\nShape of Attention Output:{attention_output.shape}\")\n",
        "print(f\"\\nAttention Output:\\n{attention_output}\") # dimension: seq_len x embedding_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMLQ2EWT01LG",
        "outputId": "d274fa81-1fbc-4592-fd14-2b9770001e74"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Shape of Attention Output:(3, 8)\n",
            "\n",
            "Attention Output:\n",
            "[[3.034  3.6074 5.1513 3.2015 3.5217 3.7557 5.0402 3.6896]\n",
            " [3.4628 3.8556 5.9131 3.1644 4.0227 3.95   4.6532 5.1311]\n",
            " [3.4618 3.8548 5.9114 3.1642 4.0219 3.9492 4.6535 5.1286]]\n"
          ]
        }
      ]
    }
  ]
}